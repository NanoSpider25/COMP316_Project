{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#PIP INSTALLS:\n",
        "!pip install wikipedia keybert sentence-transformers transformers spacy datasets evaluate matplotlib\n",
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "id": "4gtZ2XniTlou"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#IMPORTS:\n",
        "import wikipedia\n",
        "from sentence_transformers import SentenceTransformer, util\n",
        "import re\n",
        "import json\n",
        "import spacy\n",
        "from keybert import KeyBERT\n",
        "import re\n",
        "import logging\n",
        "from transformers import pipeline, AutoTokenizer\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, accuracy_score\n",
        "from google.colab import drive, files\n",
        "from datasets import load_dataset\n",
        "from tqdm import tqdm\n",
        "import evaluate\n",
        "import matplotlib.pyplot as plt\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "drive.mount(\"/content/drive\")"
      ],
      "metadata": {
        "id": "VP1AwCRAT_1V",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b2b4810b-1dfe-4c82-9ec9-1e6825d9d219"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#DEFINITIONS:\n",
        "\n",
        "# Common slang dictionary\n",
        "SLANG_DICT = {\n",
        "    \"u\": \"you\", \"ur\": \"your\", \"r\": \"are\", \"y\": \"why\",\n",
        "    \"wat\": \"what\", \"wats\": \"what is\", \"wanna\": \"want to\",\n",
        "    \"gonna\": \"going to\", \"tho\": \"though\", \"pls\": \"please\",\n",
        "    \"thx\": \"thanks\", \"lol\": \"\", \"omg\": \"oh my god\",\n",
        "    \"idk\": \"I don't know\", \"btw\": \"by the way\", \"smh\": \"shaking my head\",\n",
        "    \"bff\": \"best friend forever\", \"brb\": \"be right back\", \"tbh\": \"to be honest\",\n",
        "    \"fyi\": \"for your information\", \"lmao\": \"laughing my ass off\", \"rofl\": \"rolling on the floor laughing\",\n",
        "    \"ttyl\": \"talk to you later\", \"np\": \"no problem\", \"yolo\": \"you only live once\",\n",
        "    \"fam\": \"family\", \"lit\": \"exciting or excellent\", \"savage\": \"ruthless or harsh\",\n",
        "    \"slay\": \"to do something exceptionally well\", \"bae\": \"before anyone else\", \"cray\": \"crazy\",\n",
        "    \"fomo\": \"fear of missing out\", \"swag\": \"style or coolness\", \"vibe\": \"mood or atmosphere\",\n",
        "    \"tbh\": \"to be honest\", \"imo\": \"in my opinion\", \"srsly\": \"seriously\", \"jk\": \"just kidding\",\n",
        "    \"ppl\": \"people\", \"wyd\": \"what you doing\", \"smexy\": \"sexy\", \"af\": \"as f*ck\",\n",
        "    \"wth\": \"what the hell\", \"wtf\": \"what the f*ck\", \"asl\": \"age, sex, location\",\n",
        "    \"ik\": \"I know\", \"ikr\": \"I know, right?\", \"lml\": \"laughing madly\", \"hmu\": \"hit me up\",\n",
        "    \"mfw\": \"my face when\", \"fml\": \"f*ck my life\", \"yass\": \"yes\"\n",
        "}\n",
        "\n",
        "def extract_keyword_spacy_keybert(question):\n",
        "    doc = nlp(question)\n",
        "    question_lower = question.lower()\n",
        "\n",
        "    priority_entity_labels = [\"PERSON\", \"ORG\", \"GPE\", \"FAC\", \"LOC\"]\n",
        "    named_entities = [ent.text for ent in doc.ents if ent.label_ in priority_entity_labels]\n",
        "\n",
        "    if named_entities:\n",
        "        return named_entities[0]\n",
        "\n",
        "    noun_phrases = [chunk.text for chunk in doc.noun_chunks]\n",
        "\n",
        "    keywords = kw_model.extract_keywords(question, top_n=3, stop_words='english')\n",
        "    bert_keywords = [kw[0] for kw in keywords]\n",
        "\n",
        "\n",
        "    candidates = list(dict.fromkeys(noun_phrases + bert_keywords))\n",
        "\n",
        "    filtered_candidates = [\n",
        "        c for c in candidates\n",
        "        if c.lower() not in STOP_WORDS and len(c.strip()) > 2]\n",
        "\n",
        "\n",
        "    filtered_candidates = sorted(filtered_candidates, key=lambda x: question_lower.find(x.lower()))\n",
        "\n",
        "    return filtered_candidates[0] if filtered_candidates else question\n",
        "\n",
        "#alternative technique\n",
        "def get_wikipedia_article(query):\n",
        "    try:\n",
        "\n",
        "        try:\n",
        "            page = wikipedia.page(query, auto_suggest=False)\n",
        "            print(f\"Retrieved (exact match): {page.title}\")\n",
        "            return page.content\n",
        "        except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "            pass\n",
        "\n",
        "\n",
        "        search_results = wikipedia.search(query)\n",
        "        if not search_results:\n",
        "            print(\"No search results found.\")\n",
        "            return None\n",
        "\n",
        "\n",
        "        for title in search_results:\n",
        "            try:\n",
        "                page = wikipedia.page(title)\n",
        "                if \"replica\" not in page.title.lower():\n",
        "                    print(f\"Retrieved: {page.title}\")\n",
        "                    return page.content\n",
        "            except (wikipedia.DisambiguationError, wikipedia.PageError):\n",
        "                continue\n",
        "\n",
        "        print(\"No valid Wikipedia page found.\")\n",
        "        return None\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error occurred: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def get_best_wikipedia_article(question, top_n=5):\n",
        "    # Step 1: Search using the full question\n",
        "    search_results = wikipedia.search(question, results=top_n)\n",
        "\n",
        "\n",
        "    print(\"ðŸ”Ž Candidates:\", search_results)\n",
        "\n",
        "    # Step 2: Get summaries of each\n",
        "    summaries = []\n",
        "    titles = []\n",
        "    for title in search_results:\n",
        "        try:\n",
        "            summary = wikipedia.summary(title, sentences=3)\n",
        "            summaries.append(summary)\n",
        "            titles.append(title)\n",
        "        except wikipedia.exceptions.DisambiguationError:\n",
        "            continue\n",
        "        except wikipedia.exceptions.PageError:\n",
        "            continue\n",
        "\n",
        "    if not summaries:\n",
        "        return None\n",
        "\n",
        "    # Step 3: Use SentenceTransformer to rank similarity\n",
        "    question_embedding = embedder.encode(question, convert_to_tensor=True)\n",
        "    summary_embeddings = embedder.encode(summaries, convert_to_tensor=True)\n",
        "    cosine_scores = util.cos_sim(question_embedding, summary_embeddings)\n",
        "\n",
        "    best_idx = cosine_scores.argmax().item()\n",
        "    best_title = titles[best_idx]\n",
        "    best_page = wikipedia.page(best_title)\n",
        "\n",
        "    print(f\"âœ… Best match: {best_title}\")\n",
        "    return best_page.content\n",
        "\n",
        "def expand_question(question):\n",
        "    \"\"\"Replace common slangs and shorthand in questions.\"\"\"\n",
        "    words = question.lower().split()\n",
        "    expanded = [SLANG_DICT.get(w, w) for w in words]\n",
        "    return ' '.join(expanded)\n",
        "\n",
        "def clean_text(text):\n",
        "    \"\"\"Remove unwanted characters and normalize spacing.\"\"\"\n",
        "    text = re.sub(r'\\[\\d+\\]', '', text)  # Remove references like [1]\n",
        "    return re.sub(r'\\s+', ' ', text).strip()\n",
        "\n",
        "#alternative technique\n",
        "def chunk_text_by_tokens(text, max_tokens=450):\n",
        "    \"\"\"Split long text into chunks based on token length.\"\"\"\n",
        "    tokens = tokenizer.encode(text, add_special_tokens=False)\n",
        "    chunks = [tokens[i:i+max_tokens] for i in range(0, len(tokens), max_tokens)]\n",
        "    return [tokenizer.decode(chunk, skip_special_tokens=True) for chunk in chunks]\n",
        "\n",
        "def chunk_text_with_stride(text, max_length=384, stride=128):\n",
        "    inputs = tokenizer(text, return_overflowing_tokens=True, max_length=max_length, stride=stride, truncation=True, padding=\"max_length\")\n",
        "    return [tokenizer.decode(input_ids, skip_special_tokens=True) for input_ids in inputs[\"input_ids\"]]\n",
        "\n",
        "\n",
        "def rank_chunks_semantic(question, chunks, top_k=3):\n",
        "    question_emb = embedder.encode(question, convert_to_tensor=True)\n",
        "    chunk_embs = embedder.encode(chunks, convert_to_tensor=True)\n",
        "    scores = util.cos_sim(question_emb, chunk_embs)[0]\n",
        "    top_indices = scores.argsort(descending=True)[:top_k]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "#alternative technique\n",
        "def rank_chunks_tfidf(question, chunks, top_k=3):\n",
        "    \"\"\"Rank text chunks based on TF-IDF similarity to the question.\"\"\"\n",
        "    vectorizer = TfidfVectorizer().fit([question] + chunks)\n",
        "    vectors = vectorizer.transform([question] + chunks)\n",
        "    scores = cosine_similarity(vectors[0:1], vectors[1:]).flatten()\n",
        "    top_indices = scores.argsort()[-top_k:][::-1]\n",
        "    return [chunks[i] for i in top_indices]\n",
        "\n",
        "def is_valid_answer(answer_text, score, min_len=3, min_score=0.1):\n",
        "    \"\"\"Check if the answer is valid based on length and confidence.\"\"\"\n",
        "    return len(answer_text.strip()) >= min_len and score >= min_score\n",
        "\n",
        "def get_best_answer(question, text_chunks):\n",
        "    \"\"\"Run QA model on ranked chunks and return best answer.\"\"\"\n",
        "    answers = []\n",
        "    for chunk in text_chunks:\n",
        "        try:\n",
        "            result = qa_pipeline({'question': question, 'context': chunk})\n",
        "            if is_valid_answer(result['answer'], result['score']):\n",
        "                cleaned = re.sub(r'[^\\w\\s,.]', '', result['answer']).strip()\n",
        "                answers.append((cleaned, result['score']))\n",
        "        except Exception as e:\n",
        "            logger.warning(f\"QA failed on chunk: {e}\")\n",
        "    return max(answers, key=lambda x: x[1])[0] if answers else \"No good answer found.\"\n",
        "\n",
        "def answer_question(question, full_text):\n",
        "    \"\"\"Top-level QA function: cleans, expands, chunks, ranks, answers.\"\"\"\n",
        "    question_expanded = expand_question(question.lower())\n",
        "    cleaned_text = clean_text(full_text)\n",
        "    chunks = chunk_text_with_stride(cleaned_text)\n",
        "    top_chunks = rank_chunks_semantic(question_expanded, chunks)\n",
        "    return get_best_answer(question_expanded, top_chunks)\n",
        "\n",
        "def is_short_answer(example, max_words=3):\n",
        "    # Use the first answer in case there are multiple\n",
        "    answer = example[\"answers\"][\"text\"][0]\n",
        "    return len(answer.strip().split()) <= max_words\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FmXy5kiaUgL9",
        "outputId": "bfa42805-d5be-4d7a-e48f-677fd7b69a1f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Device set to use cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "kw_model = KeyBERT()\n",
        "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "#path = f\"/content/drive/My Drive/Distil-BERT/QA/Model{count}\"\n",
        "path = f\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/Model6\"\n",
        "qa_pipeline = pipeline(\"question-answering\", model=path, tokenizer=path)\n",
        "#tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Distil-BERT/QA/Model1\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"/content/drive/My Drive/Colab Notebooks/Distil-BERT/QA/Model1\")\n",
        "while True:\n",
        "    question = input(\"Enter your question: \")\n",
        "    if question.lower() == 'break':\n",
        "        break\n",
        "    print(\"Q:\", question)\n",
        "    article_text = get_best_wikipedia_article(extract_keyword_spacy_keybert(question))\n",
        "    print(\"A:\", answer_question(question, article_text))\n"
      ],
      "metadata": {
        "id": "EnT6P2n64rSS",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        },
        "outputId": "5a307778-9b66-4d44-a639-0c179256dc70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Enter your question: is pork haraam?\n",
            "ðŸ”Ž Candidates: ['Haram', 'Islamic finance products, services and contracts', 'Maisir', 'Nikah halala', 'Dhabihah']\n",
            "âœ… Best match: Dhabihah\n",
            "Q: is pork haraam?\n",
            "A: No good answer found.\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "SystemExit",
          "evalue": "",
          "traceback": [
            "An exception has occurred, use %tb to see the full traceback.\n",
            "\u001b[0;31mSystemExit\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/IPython/core/interactiveshell.py:3561: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
            "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
          ]
        }
      ]
    }
  ]
}